## Correlation-based Feature Selection (CFS):
  --CFS selects features based on their correlation with the target variable. Features with higher correlation values are considered more informative.
  >>>from sklearn.datasets import load_iris
  >>>from sklearn.feature_selection import SelectKBest
  >>>from sklearn.feature_selection import f_classif

  # Load the dataset
  >>>iris = load_iris()
  >>>X, y = iris.data, iris.target

  # Select top k features based on ANOVA F-statistic
  >>>k = 2
  >>>selector = SelectKBest(score_func=f_classif, k=k)
  >>>X_new = selector.fit_transform(X, y)

## Chi-squared (χ²) Test:
  --Chi-squared test measures the dependence between each feature and the target variable, and it is suitable for categorical data.
  >>>from sklearn.datasets import load_iris
  >>>from sklearn.feature_selection import SelectKBest
  >>>from sklearn.feature_selection import chi2

  # Load the dataset
  >>>iris = load_iris()
  >>>X, y = iris.data, iris.target

  # Select top k features using chi-squared test
  >>>k = 2
  >>>selector = SelectKBest(score_func=chi2, k=k)
  >>>X_new = selector.fit_transform(X, y)

## Recursive Feature Elimination (RFE):
  --RFE recursively removes the least important feature and refits the model until a desired number of features is reached.
  >>>from sklearn.datasets import load_iris
  >>>from sklearn.feature_selection import RFE
  >>>from sklearn.linear_model import LogisticRegression

  # Load the dataset
  >>>iris = load_iris()
  >>>X, y = iris.data, iris.target

  # Create a logistic regression model
  >>>model = LogisticRegression()

  # Specify the number of desired features
  >>>num_features = 2
  >>>rfe = RFE(model, num_features)
  >>>X_new = rfe.fit_transform(X, y)

## Gravitational Search Algorithm (GSA)
  --It is a population-based optimization algorithm inspired by the law of gravity and the motion of celestial bodies.

### General code
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load the dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the fitness function (e.g., classification accuracy)
def fitness_function(selected_features):
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train[:, selected_features], y_train)
    accuracy = clf.score(X_test[:, selected_features], y_test)
    return accuracy

# Initialize GSA parameters
population_size = 50
max_iterations = 100

# Main GSA loop
population = np.random.choice([0, 1], size=(population_size, X.shape[1]), replace=True)
best_solution = None
best_fitness = 0

for iteration in range(max_iterations):
    # Calculate fitness for each solution in the population
    fitness_values = [fitness_function(selected_features) for selected_features in population]
    
    # Find the best solution
    best_index = np.argmax(fitness_values)
    current_best_fitness = fitness_values[best_index]
    
    if current_best_fitness > best_fitness:
        best_solution = population[best_index]
        best_fitness = current_best_fitness
    
    # Update positions using gravitational forces (simplified example)
    # You can implement the GSA gravitational force calculation here
    
    # Apply mass redistribution (optional)
    
# The best_solution represents the optimal subset of features
print("Best Subset of Features:", best_solution)
print("Best Fitness (Accuracy):", best_fitness)

