{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#from nltk.corpus import stopwords\n",
    "import string\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4acca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the input file\n",
    "input_file = 'input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "\n",
    "# Create a directory to save the extracted articles\n",
    "output_dir = 'extracted_articles'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "\n",
    "\n",
    "# Function to extract article text\n",
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # to extract the title and article text\n",
    "        title = soup.find('title').text\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.text + '\\n'\n",
    "        return title, article_text\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Looping through the URLs in the DataFrame URL_ID\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']  \n",
    "    title, article_text = extract_article_text(url)\n",
    "    if title and article_text:\n",
    "        # Generate a unique file name using URL_ID\n",
    "        file_name = os.path.join(output_dir, f'{url_id}.txt')\n",
    "        # Write the title and article text to the file\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Title: {title}\\n\\n')\n",
    "            f.write(article_text)\n",
    "    else:\n",
    "        print(f'Error extracting: {url} with id :{url_id}')\n",
    "\n",
    "print('Extraction and saving complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(input_file):\n",
    "    '''\n",
    "    This function is making a set of stop words found in stop word folder\n",
    "    '''\n",
    "    with open('StopWords-20230811T064926Z-001\\\\StopWords\\\\'+ input_file,'r') as f:\n",
    "        content = f.read()\n",
    "        cleaned_text = re.sub(r'[\\n|]', ' ', content)\n",
    "        return cleaned_text.split()\n",
    "\n",
    "stop_words =[]\n",
    "stop_word_dir = os.listdir('StopWords-20230811T064926Z-001\\\\StopWords\\\\')\n",
    "for x in stop_word_dir:\n",
    "    stop_words = get_stop_words(x)\n",
    "\n",
    "stop_words = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(input_file_name):\n",
    "    '''\n",
    "    This function is removing stop words from text of extracted file\n",
    "    '''\n",
    "    with open('extracted_articles\\\\'+str(input_file_name) , 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        content = content.split()\n",
    "        content = [word for word in content if word.lower() not in stop_words]\n",
    "    with open('extracted_articles\\\\'+str(input_file_name) , 'w', encoding='utf-8') as file:\n",
    "        content = ' '.join(content)\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "extracted_articles = os.listdir('extracted_articles')\n",
    "for file_name in extracted_articles:\n",
    "    remove_stop_words(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "\n",
    "def process_file(filename, word_set):\n",
    "    '''\n",
    "    This function processes a file and extract words not in stopwords\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                # Remove punctuation and convert to lowercase\n",
    "                word = word.strip('.,!?()[]{}\"\\'').lower()\n",
    "                # Check if the word is not empty and word not in the stopwords set\n",
    "                if word and word not in stop_words:\n",
    "                    word_set.add(word)\n",
    "\n",
    "                    \n",
    "positive_file = 'MasterDictionary-20230811T080722Z-001\\\\MasterDictionary\\\\positive-words.txt'\n",
    "negative_file='MasterDictionary-20230811T080722Z-001\\\\MasterDictionary\\\\negative-words.txt'\n",
    "\n",
    "\n",
    "# Processing the positive and negative files\n",
    "process_file(positive_file, positive_words)\n",
    "process_file(negative_file, negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the extracted article files are located\n",
    "directory = \"extracted_articles\"\n",
    "\n",
    "# Initialize empty lists with fixed size\n",
    "num_files = len(df['URL_ID'])\n",
    "positive_scores = [0] * num_files\n",
    "negative_scores = [0] * num_files\n",
    "polarity_scores = [0] * num_files\n",
    "subjectivity_scores = [0] * num_files\n",
    "avg_sentence_lengths = [0] * num_files\n",
    "percentage_complex_words = [0] * num_files\n",
    "fog_indices = [0] * num_files\n",
    "complex_word_counts = [0] * num_files\n",
    "word_counts = [0] * num_files\n",
    "syllable_counts_per_word = [0] * num_files\n",
    "personal_pronoun_counts = [0] * num_files\n",
    "average_word_lengths = [0] * num_files\n",
    "average_words_per_sentence = [0] * num_files  \n",
    "filenames = [0]*num_files\n",
    "complex_word_counts = [0]*num_files\n",
    "\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(text, idx):\n",
    "    '''\n",
    "    Function to calculate Average Sentence Length\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    avg_sentence_lengths[idx] = len(tokens) / len(sentences)\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_fog_index(idx):\n",
    "    '''\n",
    "     Function to calculate Fog Index\n",
    "    '''\n",
    "    fog_indices[idx] = 0.4 * (avg_sentence_lengths[idx] + percentage_complex_words[idx])\n",
    "\n",
    "\n",
    "def calculate_sentiment_scores(tokens, idx):\n",
    "    '''\n",
    "    Function to calculate positive and negative scores\n",
    "    '''\n",
    "    text_positive_score = sum(1 for token in tokens if token.lower() in positive_words)\n",
    "    text_negative_score = sum(1 for token in tokens if token.lower() in negative_words)\n",
    "    \n",
    "    positive_scores[idx] = text_positive_score\n",
    "    negative_scores[idx] = text_negative_score\n",
    "    total_words = len(tokens)\n",
    "    \n",
    "    # Calculate polarity and subjectivity scores\n",
    "    text_polarity_score = (text_positive_score - text_negative_score) / (text_positive_score + text_negative_score + 0.000001)\n",
    "    text_subjectivity_score = (text_positive_score + text_negative_score) / (total_words + 0.000001)\n",
    "    polarity_scores[idx] = text_polarity_score\n",
    "    subjectivity_scores[idx] = text_subjectivity_score\n",
    "\n",
    "def calculate_personal_pronoun_count(text, idx):\n",
    "    '''\n",
    "    Function to calculate Personal Pronoun Count using regex\n",
    "    '''\n",
    "    \n",
    "    personal_pronoun_count = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', text, flags=re.IGNORECASE))\n",
    "    personal_pronoun_counts[idx] = personal_pronoun_count\n",
    "    \n",
    "\n",
    "    \n",
    "def calculate_average_word_length(tokens, idx):\n",
    "    '''\n",
    "    Function to calculate Average Word Length\n",
    "    '''\n",
    "    cleaned_words = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "    total_characters = sum(len(word) for word in cleaned_words)\n",
    "    average_word_length = total_characters / len(cleaned_words)\n",
    "    average_word_lengths[idx] = average_word_length\n",
    "\n",
    "\n",
    "def calculate_average_words_per_sentence(tokens, idx):\n",
    "    '''\n",
    "    Function to calculate Average Words Per Sentence\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    average_words_per_sentence[idx] = len(tokens) / len(sentences)\n",
    "    \n",
    "def count_syllables(word):\n",
    "    '''\n",
    "    Function to count syllables in a word\n",
    "    '''\n",
    "    if word.lower():\n",
    "        syllable_count = max([len(list(y for y in x if y[-1].isdigit())) for x in word])\n",
    "        \n",
    "        # Handle exceptions for words ending with \"es\" or \"ed\"\n",
    "        if word.lower().endswith((\"es\", \"ed\")):\n",
    "            syllable_count -= 1\n",
    "        \n",
    "        return syllable_count\n",
    "    else:\n",
    "        # Default to 1 syllable if word is not found in dictionary\n",
    "        return 1\n",
    "\n",
    "def calculate_complex_word_metrics(text, idx):\n",
    "    '''\n",
    "    Function to calculate Complex Word Count and Percentage of Complex Words\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    complex_word_count = 0  # Initialize the count of complex words\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Remove punctuation from the token\n",
    "        cleaned_token = ''.join(char for char in token if char not in string.punctuation)\n",
    "        \n",
    "        # Check if the cleaned token is a stopword\n",
    "        if cleaned_token.lower() not in stop_words:\n",
    "            num_syllables = count_syllables(cleaned_token)\n",
    "            \n",
    "            word_counts[idx] += len(cleaned_token)\n",
    "            syllable_counts_per_word[idx] += num_syllables\n",
    "            \n",
    "            # Consider words with more than two syllables as complex\n",
    "            if num_syllables > 2:\n",
    "                complex_word_count += 1\n",
    "                \n",
    "    \n",
    "    # Calculate Percentage of Complex Words\n",
    "    percentage_complex_words[idx] = (complex_word_count / len(tokens)) * 100\n",
    "\n",
    "\n",
    "# Process each URL_ID in the DataFrame\n",
    "for idx, url_id in enumerate(df['URL_ID']):\n",
    "    # Define the file path based on URL_ID\n",
    "    file_path = os.path.join(directory, f\"{url_id}.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if os.path.isfile(file_path):\n",
    "            # Read the content of the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Tokenize the text\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                \n",
    "                calculate_average_sentence_length(text, idx)\n",
    "                calculate_complex_word_metrics(text, idx)\n",
    "                calculate_fog_index(idx)\n",
    "                calculate_sentiment_scores(tokens, idx)\n",
    "                calculate_personal_pronoun_count(text, idx)\n",
    "                calculate_average_word_length(tokens, idx)\n",
    "                calculate_average_words_per_sentence(tokens, idx)\n",
    "                \n",
    "                # Store complex word count\n",
    "                complex_word_counts[idx] = calculate_complex_word_metrics(text,idx)\n",
    "                \n",
    "                # Store the filename\n",
    "                filenames[idx] = f\"{url_id}.txt\"\n",
    "                \n",
    "        else:\n",
    "            # If the file doesn't exist, append NaN values for scores and filename\n",
    "            filenames[idx] = f\"{url_id}.txt\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{file_path}': {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Create a new DataFrame with the calculated values\n",
    "data = {\n",
    "    'URL_ID': df['URL_ID'],\n",
    "    'URL': df['URL'],\n",
    "    'POSITIVE SCORE': positive_scores,\n",
    "    'NEGATIVE SCORE': negative_scores,\n",
    "    'POLARITY SCORE': polarity_scores,\n",
    "    'SUBJECTIVITY SCORE': subjectivity_scores,\n",
    "    'AVG SENTENCE LENGTH': avg_sentence_lengths,\n",
    "    'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "    'FOG INDEX': fog_indices,\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE': average_words_per_sentence,\n",
    "    'COMPLEX WORD COUNT': complex_word_counts,\n",
    "    'WORD COUNT': word_counts,\n",
    "    'SYLLABLE PER WORD': syllable_counts_per_word,\n",
    "    'PERSONAL PRONOUNS': personal_pronoun_counts,\n",
    "    'AVG WORD LENGTH': average_word_lengths\n",
    "}\n",
    "new_df = pd.DataFrame(data)\n",
    "new_df.to_excel('output.xlsx' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d5e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04258fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808f8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de33bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf3f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490b759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd0fc67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m title, article_text \u001b[38;5;241m=\u001b[39m extract_article_text(url)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mand\u001b[39;00m article_text:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Generate a unique file name using URL_ID\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir,\u001b[38;5;28mstr\u001b[39m(urls[i]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Write the title and article text to the file\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Read the input file\n",
    "url_list = list()\n",
    "with open('url.txt' , 'r') as f:\n",
    "    url_list = f.read().split('\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# Create a directory to save the extracted articles\n",
    "output_dir = 'extracted_articles'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "\n",
    "# Function to extract article text\n",
    "def extract_article_text(url):\n",
    "    '''\n",
    "    This fuction takes url and extract the Title and Paragraph of the Web page and stores\n",
    "    them into text file.\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # to extract the title and article text\n",
    "        title = soup.find('title').text\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.text + '\\n'\n",
    "        return title, article_text\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Looping through the URLs in the DataFrame URL_ID\n",
    "for url in url_list:\n",
    "    title, article_text = extract_article_text(url)\n",
    "    if title and article_text:\n",
    "        # Generate a unique file name using URL_ID\n",
    "        file_name = os.path.join(output_dir,str(urls[i].split('/')[3]+'.txt'))\n",
    "        # Write the title and article text to the file\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Title: {title}\\n\\n')\n",
    "            f.write(article_text)\n",
    "        print\n",
    "    else:\n",
    "        print(f'Error extracting: {url} with id :{url_id}')\n",
    "\n",
    "print('Extraction and saving complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b4d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dfb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
